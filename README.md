# Ada-Lovelace LoRA ðŸ‡§ðŸ‡· Arquivologia e LÃ­ngua Portuguesa

LoRA (~80 MB) aprimorada sobre **[Vicuna-7B v1.5]** para melhorar respostas em arquivologia, especialmente para aplicaÃ§Ã£o de Planos de ClassificaÃ§Ã£o e Tabelas de Temporalidades produzidos pelo Estado de SÃ£o Paulo

![banner](https://img.shields.io/badge/LoRA-Vicuna7B-blue)
![license](https://img.shields.io/badge/license-MIT-%2B-NC-green)

---

## Objetivo

Este modelo foi treinado com materiais selecionados do Arquivo PÃºblico do Estado de SÃ£o Paulo, visando aprimorar a precisÃ£o na classificaÃ§Ã£o de tipos de processo e fornecer respostas especializadas em arquivologia.

## Uso rÃ¡pido

```bash
git clone https://github.com/<usuario>/ada-lovelace-lora.git
cd ada-lovelace-lora
pip install -r requirements.txt

# precisa jÃ¡ ter o modelo base:
ollama pull vicuna:7b

# cria o modelo final no Ollama
ollama create ada-lovelace -f Modelfile

# teste:
ollama run ada-lovelace "Explique a diferenÃ§a entre prontuÃ¡rio funcional e social."
```

## Estrutura dos diretÃ³rios

```
Lovelace/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ hf_dataset/               # Dataset final no formato Hugging Face (arrow, state.json, etc)
â”‚   â”œâ”€â”€ logs/                     # Logs de processamento, extraÃ§Ã£o, treinamento
â”‚   â”œâ”€â”€ txt_clean/                # Textos extraÃ­dos e limpos dos PDFs (um .txt por documento)
â”‚   â””â”€â”€ README.md                 # DocumentaÃ§Ã£o dos dados
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ ada-lovelace-lora/        # Checkpoints do modelo LoRA treinado e tokenizer customizado
â”‚   â”‚   â””â”€â”€ checkpoint-3/         # Checkpoint mais recente (weights, optimizer, trainer states, etc)
â”‚   â”œâ”€â”€ Modelfile                 # Arquivo com receita para gerar modelo Ollama customizado
â”‚   â””â”€â”€ README.md                 # InstruÃ§Ãµes/model card do modelo custom
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ books_corpus.py           # (Exemplo) Script de manipulaÃ§Ã£o/conversÃ£o de corpus
â”‚   â”œâ”€â”€ build_dataset.py          # Script para criar dataset Hugging Face a partir dos .txt limpos
â”‚   â”œâ”€â”€ teste.py                  # Script para testar o modelo customizado (inference)
â”‚   â””â”€â”€ train_lora.py             # Script de treinamento LoRA 
â”‚
â”œâ”€â”€ .gitattributes                # ConfiguraÃ§Ãµes do Git (por exemplo, para tratar LF/CRLF, linguagens)
â”œâ”€â”€ logs_resumidos.json           # Logs resumidos ou estatÃ­sticas de treinamento/processamento
â””â”€â”€ README.md                     # DocumentaÃ§Ã£o principal do repositÃ³rio
```

## Dados
Ler  `data/README.md`.

## Treinamento (LoRA)

| ParÃ¢metro   | Valor                     |
| ----------- | ------------------------- |
| Base        | vicuna-7b-v1.5            |
| Ã‰pocas      | 1                         |
| SeqLen      | 1024                      |
| Rank (LoRA) | 16                        |
| LR          | 1e-4 (cosine)             |
| Hardware    | i9-13900H Â· RTX 4060 8 GB |
| Tempo       | 22 min GPU / 2 h CPU      |

### Passos:

```bash
python scripts/process_pdfs.py data/pdf_raw data/txt_clean
python scripts/build_dataset.py
python scripts/train_lora.py --dataset_dir hf_dataset  # GPU
# ou python scripts/train_lora.py                  # CPU
```

### AplicaÃ§Ã£o de tese:
CÃ³digo produz uma interaÃ§Ã£o de pergunta e resposta
```bash
python scripts/test.py
```

### Modelfile

```
FROM vicuna:7b
ADAPTER ada-lovelace-lora
```

## Resultados

| Prompt                          | Vicuna-7B                   | Ada-Lovelace                           |
| ------------------------------- | --------------------------- | -------------------------------------- |
| "prontuÃ¡rio funcional Ã— social" | respostas genÃ©ricas         | resposta com vocabulÃ¡rio especializado |

Taxa de acerto Vicuna-7B: 60%, 70%, 60%  
Taxa de acerto Ada-Lovelace: 86%

| AplicaÃ§Ã£o / IntegraÃ§Ã£o             | Funcionalidade                           | DescriÃ§Ã£o                                                            |
| ---------------------------------- | ---------------------------------------- | -------------------------------------------------------------------- |
| DefiniÃ§Ã£o de Tipo Documental       | Leitura pÃ¡gina a pÃ¡gina                  | Considera aspectos qualitativos que compÃµem o tipo de processo       |
| DefiniÃ§Ã£o de Processos             | Modelagem de documentos e tipos          | CriaÃ§Ã£o de templates e estruturas para diferentes fluxos documentais |
| Agente Cognitivo                   | IntegraÃ§Ã£o com IA                        | ConexÃ£o com modelos de NLP para anÃ¡lise e automaÃ§Ã£o de decisÃµes      |
| Gerenciamento de Respostas         | Controle de tokens de saÃ­da              | LimitaÃ§Ã£o de tokens para otimizaÃ§Ã£o de custos e performance          |
| OtimizaÃ§Ã£o de Modelos              | Aprimoramento via Few-Shot Learning      | AdaptaÃ§Ã£o do modelo com poucos exemplos para tarefas especÃ­ficas     |
| EspecializaÃ§Ã£o TemÃ¡tica            | Aceita overfitting controlado            | RedundÃ¢ncia em dados especializados para ganho de precisÃ£o temÃ¡tica  |
| IntegraÃ§Ã£o com Ollama (ou similar) | Facilidade de uso                        | ExportaÃ§Ã£o, manipulaÃ§Ã£o e aplicaÃ§Ã£o direta do modelo                 |
| SupervisÃ£o e ReforÃ§o Humano        | Preserva camadas arquivÃ­sticas complexas | Permite uso de bases de dados heterogÃªneas com validaÃ§Ã£o contextual  |


## LicenÃ§a

* CÃ³digo â€” NC + CC-BY-NC.  
* Pesos LoRA â€” Vicuna NC + CC-BY-NC.

## CitaÃ§Ã£o

```bibtex
@misc{rota2025adalovelace,
  title   = {Ada-Lovelace LoRA: Fine-tuning Vicuna-7B on Brazilian archival manuals},
  author  = {Rota, Alesson Ramon.},
  year    = {2025},
  url     = {https://github.com/<usuario>/ada-lovelace-lora}
}
```
